{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ja-pa/ea-openai-evals/blob/main/OAI_evals_up.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook pro testovani evalu\n"
      ],
      "metadata": {
        "id": "oZ9kfL5_S22d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title OpenAI API key\n",
        "\n",
        "\n",
        "openai_key = '' #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "8mJu5OHUTAps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "Tato cast stahne evals repo a nainstaluje potrebne python balicky"
      ],
      "metadata": {
        "id": "d4WvLIVQirkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/openai/evals.git\n",
        "!cd evals && git lfs fetch --all && git lfs pull\n",
        "#!cat evals/evals/registry/data/logic/samples.jsonl\n",
        "!cd evals && pip install -e .\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "from google.colab import data_table\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def display_table_from_file(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            # Parse each line as a JSON object\n",
        "            json_line = json.loads(line)\n",
        "\n",
        "            # Extract 'content' from 'input' and 'ideal' fields\n",
        "            content = json_line['input'][0]['content']\n",
        "            ideal = json_line['ideal']\n",
        "\n",
        "            # Append to data list\n",
        "            data.append({'Content': content, 'Ideal': ideal})\n",
        "\n",
        "    # Convert the list to a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Return the DataFrame\n",
        "    return df\n",
        "\n",
        "def get_latest_file(path, pattern):\n",
        "    list_of_files = glob.glob(os.path.join(path, pattern))\n",
        "    if not list_of_files:\n",
        "        return None\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    return latest_file\n",
        "\n",
        "\n",
        "#log_output=\"/tmp/evallogs/230530205047OSMJXQ5A_gpt-3.5-turbo_logic-fact.jsonl\"\n",
        "def print_log(log_output):\n",
        "  with open(log_output,\"r\") as fp:\n",
        "    for line in fp.readlines():\n",
        "      json_object = json.loads(line)\n",
        "      json_formatted_str = json.dumps(json_object, indent=2)\n",
        "      print(json_formatted_str)"
      ],
      "metadata": {
        "id": "zg9qgkzUVX4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vyhodnoceni evalu"
      ],
      "metadata": {
        "id": "Hl6q-TgItrDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spusti vyhodnoceni evalu pro logic-fact (viz https://github.com/openai/evals/blob/main/evals/registry/evals/logic.yaml)\n",
        "\n",
        "U logic-fact je eval_type **cot_classify**\n",
        "\n",
        "viz https://github.com/openai/evals/blob/main/docs/eval-templates.md\n",
        ">\"cot_classify\" (\"chain-of-thought then classify\", i.e., reason then answer) expects that the parsable portion of the response (i.e., the portion containing the choice) will be at the end of the completion. We recommend this as the default as it typically provides the most accurate model-graded evaluations.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SKSNYReXkGFQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVNCSi1lIW7S"
      },
      "outputs": [],
      "source": [
        "!cd evals && OPENAI_API_KEY=$openai_key oaieval gpt-3.5-turbo logic-fact"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zobrazi evaly pro logic ve forme tabulky"
      ],
      "metadata": {
        "id": "xaL5tqC6kXo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function with your file path and print the DataFrame\n",
        "df = display_table_from_file('evals/evals/registry/data/logic/samples.jsonl')\n",
        "display(df)\n"
      ],
      "metadata": {
        "id": "aZnQTUFThfq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zobrazi posledni vygenerovany log z **oaieval** pro logic-fact"
      ],
      "metadata": {
        "id": "M07ZtZ4Skl4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "log_output=get_latest_file(\"/tmp/evallogs\", \"*gpt-3.5-turbo_logic-fact.jsonl\")\n",
        "print(\"Log name:\", log_output)\n",
        "print_log(log_output)"
      ],
      "metadata": {
        "id": "T8hxhIKReNOG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}